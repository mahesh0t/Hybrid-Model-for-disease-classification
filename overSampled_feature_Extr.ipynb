{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fbabc1-e594-42a7-9165-5d74c6569870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from class: 1. Eczema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1. Eczema:   3%|▎         | 49/1500 [00:00<00:12, 120.70it/s]C:\\Users\\RYZEN\\AppData\\Local\\Temp\\ipykernel_12356\\2665905623.py:36: RuntimeWarning: divide by zero encountered in log10\n",
      "  log_moments = -np.sign(moments) * np.log10(np.abs(moments))  # Log scale for better representation\n",
      "C:\\Users\\RYZEN\\AppData\\Local\\Temp\\ipykernel_12356\\2665905623.py:36: RuntimeWarning: invalid value encountered in multiply\n",
      "  log_moments = -np.sign(moments) * np.log10(np.abs(moments))  # Log scale for better representation\n",
      "Processing 1. Eczema: 100%|██████████| 1500/1500 [00:12<00:00, 118.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from class: 2. Melanoma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2. Melanoma: 100%|██████████| 1000/1000 [00:08<00:00, 113.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from class: 3. Atopic Dermatitis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 3. Atopic Dermatitis: 100%|██████████| 1257/1257 [00:10<00:00, 125.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from class: 4. Melanocytic Nevi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4. Melanocytic Nevi: 100%|██████████| 1000/1000 [00:08<00:00, 124.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from class: 5. Benign Keratosis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 5. Benign Keratosis: 100%|██████████| 1000/1000 [00:08<00:00, 122.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from class: 6. Fungal Infections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 6. Fungal Infections: 100%|██████████| 1500/1500 [00:12<00:00, 122.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from class: 7. Viral Infections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 7. Viral Infections: 100%|██████████| 1500/1500 [00:12<00:00, 124.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete! Features saved in ovs_manual_features.npy and labels in ovs_manual_labels.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.measure import moments_hu\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to extract color features (mean and std of RGB channels)\n",
    "def extract_color_features(image):\n",
    "    mean = np.mean(image, axis=(0, 1))  # Mean per channel\n",
    "    std = np.std(image, axis=(0, 1))    # Std deviation per channel\n",
    "    return np.concatenate([mean, std])\n",
    "\n",
    "# Function to extract texture features using GLCM\n",
    "def extract_texture_features(image):\n",
    "    gray_image = rgb2gray(image)  # Convert to grayscale\n",
    "    gray_image = (gray_image * 255).astype(np.uint8)  # Scale to 8-bit\n",
    "    \n",
    "    glcm = graycomatrix(gray_image, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "    \n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]\n",
    "    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "    energy = graycoprops(glcm, 'energy')[0, 0]\n",
    "    correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
    "    \n",
    "    return np.array([contrast, dissimilarity, homogeneity, energy, correlation])\n",
    "\n",
    "# Function to extract shape features using Hu Moments\n",
    "def extract_shape_features(image):\n",
    "    gray_image = rgb2gray(image)  # Convert to grayscale\n",
    "    gray_image = (gray_image * 255).astype(np.uint8)  # Convert to 8-bit\n",
    "\n",
    "    moments = moments_hu(gray_image)\n",
    "    log_moments = -np.sign(moments) * np.log10(np.abs(moments))  # Log scale for better representation\n",
    "    return log_moments\n",
    "\n",
    "# Paths for input dataset and output files\n",
    "input_dir = \"Resized_IMG_CLASSES_2\"\n",
    "features_output_file = \"ovs_manual_features.npy\"\n",
    "labels_output_file = \"ovs_manual_labels.npy\"\n",
    "\n",
    "# Prepare data storage\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Process images\n",
    "for class_name in os.listdir(input_dir):\n",
    "    class_path = os.path.join(input_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue  # Skip non-folder items\n",
    "\n",
    "    print(f\"Extracting features from class: {class_name}\")\n",
    "\n",
    "    for img_name in tqdm(os.listdir(class_path), desc=f\"Processing {class_name}\"):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue  # Skip unreadable images\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Extract features\n",
    "        color_features = extract_color_features(image)\n",
    "        texture_features = extract_texture_features(image)\n",
    "        shape_features = extract_shape_features(image)\n",
    "\n",
    "        # Combine all features\n",
    "        features = np.concatenate([color_features, texture_features, shape_features])\n",
    "        features_list.append(features)\n",
    "        labels_list.append(class_name)  # Store label\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "# Save as .npy files\n",
    "np.save(features_output_file, features_array)\n",
    "np.save(labels_output_file, labels_array)\n",
    "\n",
    "print(f\"Feature extraction complete! Features saved in {features_output_file} and labels in {labels_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05f8650-391c-4b2b-b01e-6f0f38a2aa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Extracting DenseNet121 features: 100%|██████████| 274/274 [07:21<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet121 features saved to: oversampled_featuresets/ovs_densenet_features.npy\n",
      "Labels saved to: oversampled_featuresets/ovs_densenet_labels.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "data_dir = \"oversampled_normalized_images/densenet\"\n",
    "output_feature_path = \"oversampled_featuresets/ovs_densenet_features.npy\"\n",
    "output_label_path = \"oversampled_featuresets/ovs_densenet_labels.npy\"\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset\n",
    "class DenseNetImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = sorted(os.listdir(root_dir))\n",
    "        for label_index, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                self.labels.append(label_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = DenseNetImageDataset(data_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DenseNet121 and remove classifier\n",
    "densenet = models.densenet121(pretrained=True)\n",
    "densenet.classifier = torch.nn.Identity()  # Remove final classification layer\n",
    "densenet = densenet.to(device)\n",
    "densenet.eval()\n",
    "\n",
    "# Extract features\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(dataloader, desc=\"Extracting DenseNet121 features\"):\n",
    "        images = images.to(device)\n",
    "        outputs = densenet(images)\n",
    "        features_list.append(outputs.cpu().numpy())\n",
    "        labels_list.extend(labels.numpy())\n",
    "\n",
    "# Save features and labels\n",
    "features_array = np.vstack(features_list)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "np.save(output_feature_path, features_array)\n",
    "np.save(output_label_path, labels_array)\n",
    "\n",
    "print(\"DenseNet121 features saved to:\", output_feature_path)\n",
    "print(\"Labels saved to:\", output_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac23ecb-6fc3-48c1-962d-af691751eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Extracting MobileNetV2 features: 100%|██████████| 274/274 [03:12<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 features saved to: oversampled_featuresets/ovs_mobilenet_features.npy\n",
      "Labels saved to: oversampled_featuresets/ovs_mobilenet_labels.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "data_dir = \"oversampled_normalized_images/mobilenetv2\"\n",
    "output_feature_path = \"oversampled_featuresets/ovs_mobilenet_features.npy\"\n",
    "output_label_path = \"oversampled_featuresets/ovs_mobilenet_labels.npy\"\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset\n",
    "class MobileNetImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = sorted(os.listdir(root_dir))\n",
    "        for label_index, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                self.labels.append(label_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = MobileNetImageDataset(data_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load MobileNetV2 and remove classifier\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "mobilenet.classifier = torch.nn.Identity()  # Remove classification layer\n",
    "mobilenet = mobilenet.to(device)\n",
    "mobilenet.eval()\n",
    "\n",
    "# Extract features\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(dataloader, desc=\"Extracting MobileNetV2 features\"):\n",
    "        images = images.to(device)\n",
    "        outputs = mobilenet(images)\n",
    "        features_list.append(outputs.cpu().numpy())\n",
    "        labels_list.extend(labels.numpy())\n",
    "\n",
    "# Save features and labels\n",
    "features_array = np.vstack(features_list)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "np.save(output_feature_path, features_array)\n",
    "np.save(output_label_path, labels_array)\n",
    "\n",
    "print(\"MobileNetV2 features saved to:\", output_feature_path)\n",
    "print(\"Labels saved to:\", output_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d69643-d8b1-4d32-ba43-d5dc94f59d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Extracting ResNet50 features: 100%|██████████| 274/274 [07:24<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 features saved to: oversampled_featuresets/ovs_resnet_features.npy\n",
      "Labels saved to: oversampled_featuresets/ovs_resnet_labels.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "data_dir = \"oversampled_normalized_images/resnet\"\n",
    "output_feature_path = \"oversampled_featuresets/ovs_resnet_features.npy\"\n",
    "output_label_path = \"oversampled_featuresets/ovs_resnet_labels.npy\"\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset\n",
    "class ResNetImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = sorted(os.listdir(root_dir))\n",
    "        for label_index, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                self.labels.append(label_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = ResNetImageDataset(data_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load ResNet50 and remove the final FC layer\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet.fc = torch.nn.Identity()  # Remove classification layer\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# Extract features\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(dataloader, desc=\"Extracting ResNet50 features\"):\n",
    "        images = images.to(device)\n",
    "        outputs = resnet(images)\n",
    "        features_list.append(outputs.cpu().numpy())\n",
    "        labels_list.extend(labels.numpy())\n",
    "\n",
    "# Save features and labels\n",
    "features_array = np.vstack(features_list)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "np.save(output_feature_path, features_array)\n",
    "np.save(output_label_path, labels_array)\n",
    "\n",
    "print(\"ResNet50 features saved to:\", output_feature_path)\n",
    "print(\"Labels saved to:\", output_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16566fb1-8c02-4a66-a165-fa1e604f1897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
