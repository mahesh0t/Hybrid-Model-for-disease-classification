{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be118a0e-42a7-4a49-b47a-590e0a7af3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG features: (7000, 25088)\n",
      "Manual features: (7000, 18)\n",
      "Labels: (7000,)\n",
      "Combined features shape: (7000, 25106)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.56      0.51       200\n",
      "           1       0.70      0.85      0.77       200\n",
      "           2       0.38      0.33      0.35       200\n",
      "           3       0.67      0.78      0.72       200\n",
      "           4       0.80      0.66      0.72       200\n",
      "           5       0.41      0.47      0.44       200\n",
      "           6       0.51      0.33      0.40       200\n",
      "\n",
      "    accuracy                           0.56      1400\n",
      "   macro avg       0.56      0.57      0.56      1400\n",
      "weighted avg       0.56      0.56      0.56      1400\n",
      "\n",
      "RF Accuracy: 0.565\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load features and labels\n",
    "vgg_features = np.load(\"featuresets/vgg_features.npy\")\n",
    "manual_features = np.load(\"featuresets/manual_features.npy\")\n",
    "labels = np.load(\"featuresets/vgg_labels.npy\")\n",
    "\n",
    "# Confirm shapes\n",
    "print(\"VGG features:\", vgg_features.shape)\n",
    "print(\"Manual features:\", manual_features.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "\n",
    "# Concatenate VGG and manual features\n",
    "combined_features = np.hstack((vgg_features, manual_features))\n",
    "print(\"Combined features shape:\", combined_features.shape)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined_features, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"RF Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "924f99e3-2a91-473b-b194-30708e296a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG features: (7000, 25088)\n",
      "Manual features: (7000, 18)\n",
      "Labels: (7000,)\n",
      "Combined feature shape: (7000, 25106)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.57      0.52       203\n",
      "           1       0.85      0.80      0.82       224\n",
      "           2       0.39      0.38      0.38       178\n",
      "           3       0.73      0.71      0.72       215\n",
      "           4       0.72      0.81      0.77       183\n",
      "           5       0.46      0.46      0.46       211\n",
      "           6       0.40      0.32      0.35       186\n",
      "\n",
      "    accuracy                           0.59      1400\n",
      "   macro avg       0.58      0.58      0.58      1400\n",
      "weighted avg       0.58      0.59      0.58      1400\n",
      "\n",
      "SVM Accuracy: 0.5864285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "vgg_features = np.load(\"featuresets/vgg_features.npy\")\n",
    "manual_features = np.load(\"featuresets/manual_features.npy\")\n",
    "labels = np.load(\"featuresets/vgg_labels.npy\")\n",
    "\n",
    "# Check dimensions\n",
    "print(\"VGG features:\", vgg_features.shape)\n",
    "print(\"Manual features:\", manual_features.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "\n",
    "# Concatenate features\n",
    "combined_features = np.hstack((vgg_features, manual_features))\n",
    "print(\"Combined feature shape:\", combined_features.shape)\n",
    "\n",
    "# Handle missing values using imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "combined_features_imputed = imputer.fit_transform(combined_features)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features_imputed, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVM classifier\n",
    "model = SVC(kernel='rbf', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25fa9b76-c309-42a1-9936-e4536d031e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG features: (7000, 25088)\n",
      "Manual features: (7000, 18)\n",
      "Labels: (7000,)\n",
      "Combined feature shape: (7000, 25106)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.51      0.54       200\n",
      "           1       0.83      0.90      0.86       200\n",
      "           2       0.46      0.41      0.43       200\n",
      "           3       0.77      0.78      0.77       200\n",
      "           4       0.78      0.80      0.79       200\n",
      "           5       0.51      0.57      0.54       200\n",
      "           6       0.52      0.51      0.52       200\n",
      "\n",
      "    accuracy                           0.64      1400\n",
      "   macro avg       0.63      0.64      0.64      1400\n",
      "weighted avg       0.63      0.64      0.64      1400\n",
      "\n",
      "Logostic Regression Accuracy: 0.6385714285714286\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load features and labels\n",
    "vgg_features = np.load(\"featuresets/vgg_features.npy\")\n",
    "manual_features = np.load(\"featuresets/manual_features.npy\")\n",
    "labels = np.load(\"featuresets/vgg_labels.npy\")\n",
    "\n",
    "# Check dimensions\n",
    "print(\"VGG features:\", vgg_features.shape)\n",
    "print(\"Manual features:\", manual_features.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "\n",
    "# Concatenate features\n",
    "combined_features = np.hstack((vgg_features, manual_features))\n",
    "print(\"Combined feature shape:\", combined_features.shape)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "combined_features = imputer.fit_transform(combined_features)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "combined_features = scaler.fit_transform(combined_features)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluation\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Logostic Regression Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c110f92c-7d40-4a07-86e6-9d6efd6c2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG features: (7000, 25088)\n",
      "Manual features: (7000, 18)\n",
      "Labels: (7000,)\n",
      "Combined feature shape: (7000, 25106)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.43      0.43       200\n",
      "           1       0.61      0.63      0.62       200\n",
      "           2       0.34      0.32      0.32       200\n",
      "           3       0.57      0.56      0.56       200\n",
      "           4       0.61      0.60      0.61       200\n",
      "           5       0.34      0.29      0.31       200\n",
      "           6       0.29      0.34      0.31       200\n",
      "\n",
      "    accuracy                           0.45      1400\n",
      "   macro avg       0.45      0.45      0.45      1400\n",
      "weighted avg       0.45      0.45      0.45      1400\n",
      "\n",
      "Decision Tree Accuracy: 0.45285714285714285\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load features and labels\n",
    "vgg_features = np.load(\"featuresets/vgg_features.npy\")\n",
    "manual_features = np.load(\"featuresets/manual_features.npy\")\n",
    "labels = np.load(\"featuresets/vgg_labels.npy\")\n",
    "\n",
    "# Check dimensions\n",
    "print(\"VGG features:\", vgg_features.shape)\n",
    "print(\"Manual features:\", manual_features.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "\n",
    "# Concatenate features\n",
    "combined_features = np.hstack((vgg_features, manual_features))\n",
    "print(\"Combined feature shape:\", combined_features.shape)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "combined_features = imputer.fit_transform(combined_features)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "combined_features = scaler.fit_transform(combined_features)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Train Decision Tree\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c3a8fab-55ed-4d7b-a998-e839a2cec4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG features: (7000, 25088)\n",
      "Manual features: (7000, 18)\n",
      "Labels: (7000,)\n",
      "Combined feature shape: (7000, 25106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.27      0.26       200\n",
      "           1       0.66      0.44      0.53       200\n",
      "           2       0.31      0.20      0.24       200\n",
      "           3       0.70      0.12      0.20       200\n",
      "           4       0.32      0.83      0.46       200\n",
      "           5       0.33      0.12      0.17       200\n",
      "           6       0.23      0.35      0.28       200\n",
      "\n",
      "    accuracy                           0.33      1400\n",
      "   macro avg       0.40      0.33      0.31      1400\n",
      "weighted avg       0.40      0.33      0.31      1400\n",
      "\n",
      "KNN Accuracy: 0.3314285714285714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load features and labels\n",
    "vgg_features = np.load(\"featuresets/vgg_features.npy\")\n",
    "manual_features = np.load(\"featuresets/manual_features.npy\")\n",
    "labels = np.load(\"featuresets/vgg_labels.npy\")\n",
    "\n",
    "# Check dimensions\n",
    "print(\"VGG features:\", vgg_features.shape)\n",
    "print(\"Manual features:\", manual_features.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "\n",
    "# Concatenate features\n",
    "combined_features = np.hstack((vgg_features, manual_features))\n",
    "print(\"Combined feature shape:\", combined_features.shape)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "combined_features = imputer.fit_transform(combined_features)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "combined_features = scaler.fit_transform(combined_features)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Train KNN\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fea617fa-5668-44ee-9c3f-aa51c3ac22ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG features: (7000, 25088)\n",
      "Manual features: (7000, 18)\n",
      "Labels: (7000,)\n",
      "Combined feature shape: (7000, 25106)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.35      0.35       200\n",
      "           1       0.49      0.45      0.47       200\n",
      "           2       0.31      0.32      0.31       200\n",
      "           3       0.40      0.30      0.35       200\n",
      "           4       0.59      0.28      0.38       200\n",
      "           5       0.32      0.49      0.39       200\n",
      "           6       0.22      0.29      0.25       200\n",
      "\n",
      "    accuracy                           0.36      1400\n",
      "   macro avg       0.38      0.36      0.36      1400\n",
      "weighted avg       0.38      0.36      0.36      1400\n",
      "\n",
      "Naive Bayes Accuracy: 0.3557142857142857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load features and labels\n",
    "vgg_features = np.load(\"featuresets/vgg_features.npy\")\n",
    "manual_features = np.load(\"featuresets/manual_features.npy\")\n",
    "labels = np.load(\"featuresets/vgg_labels.npy\")\n",
    "\n",
    "# Check dimensions\n",
    "print(\"VGG features:\", vgg_features.shape)\n",
    "print(\"Manual features:\", manual_features.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "\n",
    "# Concatenate features\n",
    "combined_features = np.hstack((vgg_features, manual_features))\n",
    "print(\"Combined feature shape:\", combined_features.shape)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "combined_features = imputer.fit_transform(combined_features)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "combined_features = scaler.fit_transform(combined_features)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Train Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146c744e-9ff4-4086-b791-c8d23160b2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG features: (7000, 25088)\n",
      "Manual features: (7000, 18)\n",
      "Labels: (7000,)\n",
      "Combined feature shape: (7000, 25106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:28:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[23:28:12] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 4709570688 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train XGBoost classifier\u001b[39;00m\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m     33\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\xgboost\\sklearn.py:1682\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1660\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1661\u001b[0m     xgb_model, params, feature_weights\n\u001b[0;32m   1662\u001b[0m )\n\u001b[0;32m   1663\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1664\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1665\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1680\u001b[0m )\n\u001b[1;32m-> 1682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1683\u001b[0m     params,\n\u001b[0;32m   1684\u001b[0m     train_dmatrix,\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1686\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   1687\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1688\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[0;32m   1689\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[0;32m   1690\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   1691\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1692\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1693\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1694\u001b[0m )\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\xgboost\\training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\xgboost\\core.py:2246\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2246\u001b[0m     _check_call(\n\u001b[0;32m   2247\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[0;32m   2248\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   2249\u001b[0m         )\n\u001b[0;32m   2250\u001b[0m     )\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\xgboost\\core.py:310\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [23:28:12] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 4709570688 bytes."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load features and labels\n",
    "vgg_features = np.load(\"featuresets/vgg_features.npy\")\n",
    "manual_features = np.load(\"featuresets/manual_features.npy\")\n",
    "labels = np.load(\"featuresets/vgg_labels.npy\")\n",
    "\n",
    "# Check dimensions\n",
    "print(\"VGG features:\", vgg_features.shape)\n",
    "print(\"Manual features:\", manual_features.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "\n",
    "# Concatenate features\n",
    "combined_features = np.hstack((vgg_features, manual_features))\n",
    "print(\"Combined feature shape:\", combined_features.shape)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "combined_features = imputer.fit_transform(combined_features)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost classifier\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe705f-1b67-45cc-abe5-7fe00a313bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
